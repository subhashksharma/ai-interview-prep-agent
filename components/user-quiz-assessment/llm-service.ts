// Mock LLM Service for Assessment Generation
// In production, replace this with actual API calls to your LLM backend

import {
  Question,
  TopicOption,
  CustomAssessmentInput,
  DifficultyLevel,
  AssessmentFeedback,
  Answer,
} from './types';

// Simulated delay to mimic API call
const simulateDelay = (ms: number = 1500) => new Promise((resolve) => setTimeout(resolve, ms));

/**
 * Generate questions based on a selected topic
 * In production: Call your LLM API with the topic and difficulty
 */
export async function generateTopicQuestions(
  topic: TopicOption,
  difficulty: DifficultyLevel
): Promise<Question[]> {
  await simulateDelay();

  // This is mock data. In production, your LLM would generate these dynamically
  const questions: Question[] = [];

  for (let i = 0; i < topic.questionCount; i++) {
    questions.push({
      id: `${topic.id}-q${i + 1}`,
      question: `Question ${i + 1} about ${
        topic.name
      }: [Generated by LLM based on difficulty: ${difficulty}]`,
      type:
        i % 4 === 0
          ? 'multiple-choice'
          : i % 4 === 1
          ? 'coding'
          : i % 4 === 2
          ? 'scenario-based'
          : 'open-ended',
      difficulty,
      timeEstimate: 180 + Math.floor(Math.random() * 300),
      category: topic.category,
      hints: [
        'Consider the fundamental concepts',
        'Think about real-world applications',
        'Break the problem into smaller parts',
      ].slice(0, Math.floor(Math.random() * 3) + 1),
      options:
        i % 4 === 0
          ? [
              'Option A: [Generated by LLM]',
              'Option B: [Generated by LLM]',
              'Option C: [Generated by LLM]',
              'Option D: [Generated by LLM]',
            ]
          : undefined,
    });
  }

  return questions;
}

/**
 * Generate custom questions based on user input
 * In production: Call your LLM API with the custom requirements
 */
export async function generateCustomQuestions(input: CustomAssessmentInput): Promise<Question[]> {
  await simulateDelay(2000);

  // Build context for LLM
  const context = `
    Topic: ${input.topic}
    ${input.specificAreas ? `Specific Areas: ${input.specificAreas.join(', ')}` : ''}
    Difficulty: ${input.difficulty}
    Number of Questions: ${input.questionCount}
    Duration: ${input.duration} minutes
    ${input.additionalContext ? `Additional Context: ${input.additionalContext}` : ''}
  `;

  console.log('LLM Context:', context);

  // Mock questions - replace with actual LLM generation
  const questions: Question[] = [];

  for (let i = 0; i < input.questionCount; i++) {
    questions.push({
      id: `custom-q${i + 1}`,
      question: `Custom question ${i + 1} about ${input.topic} [Generated by LLM at ${
        input.difficulty
      } level]`,
      type: i % 3 === 0 ? 'open-ended' : i % 3 === 1 ? 'scenario-based' : 'coding',
      difficulty: input.difficulty,
      timeEstimate: Math.floor((input.duration * 60) / input.questionCount),
      category: 'custom',
      hints: [`Hint related to ${input.topic}`, 'Consider your specific context'],
    });
  }

  return questions;
}

/**
 * Evaluate an answer and provide feedback
 * In production: Call your LLM API to analyze the answer
 */
export async function evaluateAnswer(
  question: Question,
  answer: Answer
): Promise<AssessmentFeedback> {
  await simulateDelay(1000);

  // Mock feedback - replace with actual LLM evaluation
  const isPositive = Math.random() > 0.3; // 70% positive feedback

  return {
    questionId: question.id,
    feedback: `[LLM-generated feedback for your answer about: ${question.question.substring(
      0,
      50
    )}...]`,
    strengths: [
      'Clear explanation of concepts',
      'Good use of examples',
      'Demonstrated understanding',
    ].slice(0, Math.floor(Math.random() * 3) + 1),
    areasForImprovement: [
      'Could expand on edge cases',
      'Consider performance implications',
      'Add more specific details',
    ].slice(0, Math.floor(Math.random() * 2)),
    encouragement: isPositive
      ? "Great work! You're showing solid understanding. Keep it up! ðŸŒŸ"
      : "Good effort! Every attempt is a learning opportunity. You're making progress! ðŸ’ª",
    nextSteps: [
      'Review the official documentation',
      'Practice similar problems',
      'Explore advanced concepts',
    ],
    isPositive,
  };
}

/**
 * Generate a comprehensive assessment report
 * In production: Call your LLM API to analyze all answers and generate insights
 */
export async function generateAssessmentReport(
  questions: Question[],
  answers: Answer[]
): Promise<{
  overallScore: number;
  strengths: string[];
  areasForImprovement: string[];
  recommendations: string[];
  nextSteps: string[];
}> {
  await simulateDelay(2000);

  return {
    overallScore: 75 + Math.floor(Math.random() * 20),
    strengths: [
      'Strong foundational knowledge',
      'Good problem-solving approach',
      'Clear communication of ideas',
    ],
    areasForImprovement: [
      'Could improve on advanced concepts',
      'Practice more complex scenarios',
      'Focus on optimization techniques',
    ],
    recommendations: [
      'Review documentation for advanced features',
      'Practice coding challenges regularly',
      'Join study groups or communities',
    ],
    nextSteps: [
      'Take the next level assessment',
      'Work on a real-world project',
      'Explore related technologies',
    ],
  };
}

/**
 * Example API integration structure for production:
 *
 * export async function generateTopicQuestions(
 *   topic: TopicOption,
 *   difficulty: DifficultyLevel
 * ): Promise<Question[]> {
 *   const response = await fetch('/api/generate-questions', {
 *     method: 'POST',
 *     headers: { 'Content-Type': 'application/json' },
 *     body: JSON.stringify({
 *       topic: topic.name,
 *       category: topic.category,
 *       difficulty,
 *       questionCount: topic.questionCount,
 *       tags: topic.tags,
 *     }),
 *   });
 *
 *   const data = await response.json();
 *   return data.questions;
 * }
 */
